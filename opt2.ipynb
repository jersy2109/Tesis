{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, env):\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Realiza un número aleatorio de movimientos \"NOOP\" al reiniciar el ambiente.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "        assert self.env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "        \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        noops = self.env.unwrapped.np_random.integers(1, self.noop_max+1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Salta un número de frames y regresa el valor máximo de cada pixel.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._skip = skip\n",
    "        self._obs_buffer = np.zeros((self._skip,) + self.env.observation_space.shape, dtype=np.uint8)\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, _, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs            \n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimit(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Termina el episodio después de un determinado número de pasos.\n",
    "    Evita que los ambientes se mantengan en un loop o sin jugar. \n",
    "    \"\"\"   \n",
    "    def __init__(self, env, max_episode_steps=None):\n",
    "        super(TimeLimit, self).__init__(env)\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            done = True\n",
    "            info['TimeLimit.truncated'] = True\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._elapsed_steps = 0\n",
    "        return self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Realiza la acción \"FIRE\" para iniciar los juegos que así lo requieran.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = self.env.ale.lives()\n",
    "        assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(self.env.unwrapped.get_action_meanings()) >= 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.lives > self.env.ale.lives():\n",
    "            self.lives = self.env.ale.lives()\n",
    "            action = 1\n",
    "        return self.env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Trunca las recompensas obtenidas a valores entre -1 a 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, min_r=-1, max_r=1):\n",
    "        super().__init__(env)\n",
    "        self.min_r = min_r\n",
    "        self.max_r = max_r\n",
    "        \n",
    "    def reward(self, reward):\n",
    "        if reward < 0:\n",
    "            return self.min_r\n",
    "        elif reward > 0:\n",
    "            return self.max_r\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        return obs, self.reward(rew), done, info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reescala las imágenes a 84x84 y las convierte a escala de grises.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, width=84, height=84):\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        num_colors = 1\n",
    "        \n",
    "        new_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255,\n",
    "            shape = (self._height, self._width, num_colors),\n",
    "            dtype = np.uint8,\n",
    "        )\n",
    "        original_space = self.observation_space\n",
    "        self.observation_space = new_space\n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        frame = cv.cvtColor(obs, cv.COLOR_RGB2GRAY)\n",
    "        return cv.resize(\n",
    "            frame, (self._width, self._height), interpolation=cv.INTER_AREA\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return self.observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFLoatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reescala los valores de los pixeles de 0-255 a 0-1.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255, \n",
    "            shape = self.env.observation_space.shape,\n",
    "            dtype = np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return self.observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    De los más recientes k frames observados, devuelve los últimos 2 apilados.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, k=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=2)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255,\n",
    "            shape = (2,84,84),\n",
    "            dtype = self.env.observation_space.dtype,\n",
    "        )\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Termina el episodio cuando se pierde una vida, pero solo reinicia si\n",
    "    se pierden todas.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.is_done = True\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.is_done = done\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.is_done:\n",
    "            obs = self.env.reset()\n",
    "        else:\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari(env_id, frames=4, max_steps=1_000, noop_max=30, skip=4, sample=False, render_mode=None):\n",
    "    \"\"\"\n",
    "    Crea el ambiente con los parámetros especificados.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max)\n",
    "    env = MaxAndSkipEnv(env, skip)\n",
    "    if max_steps is not None:\n",
    "        env = TimeLimit(env, max_steps)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    if not sample:\n",
    "        env = ClipReward(env)\n",
    "    env = WarpFrame(env)\n",
    "    env = ScaledFLoatFrame(env)\n",
    "    env = FrameStack(env, frames)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    return env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Profunda de Aprendizaje Q (Deep Q Network).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1,*shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \"\"\"\n",
    "    Almacena experiencias pasadas que han sido observadas por el agente.\n",
    "    Las muestras obtenidas sirven para entrenar la red, buscando minimizar el efecto que tiene la correlación entre pasos.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, *args):\n",
    "        self.buffer.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    El agente que se encarga de jugar.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = self.optical_flow(self.env.reset())\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device='cuda'):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device) \n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1) # Devuelve el índice de la acción\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        new_state = self.optical_flow(new_state)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.exp_buffer.append(self.state, action, reward, done, new_state)\n",
    "        self.state = new_state\n",
    "\n",
    "        if done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        \n",
    "        return done_reward\n",
    "\n",
    "    def optical_flow(self, obs):\n",
    "        assert np.array(obs).shape == (2, 84, 84)\n",
    "\n",
    "        first_frame = np.array(obs)[0].astype('uint8')\n",
    "        #prev_gray = cv.cvtColor(first_frame, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "        mask = np.zeros((84,84,3))\n",
    "        mask[..., 1] = 255\n",
    "\n",
    "        frame = np.array(obs)[1].astype('uint8')\n",
    "        #gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "        flow = cv.calcOpticalFlowFarneback(first_frame, frame,\n",
    "                                        None, \n",
    "                                        0.5, 5, 5, 5, 7, 1.5, 0)\n",
    "\n",
    "        magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "        mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
    "\n",
    "        flow = cv.cvtColor(mask.astype('float32'), cv.COLOR_HSV2RGB)\n",
    "\n",
    "        final = np.zeros((4,84,84))\n",
    "        final[0:3] = flow.reshape(3,84,84)\n",
    "        final[3] = np.array(obs[1]) / 255.0\n",
    "\n",
    "        assert final.shape == (4,84,84)\n",
    "\n",
    "        return final.astype(\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_stopping(timer):\n",
    "    delta = datetime.timedelta(seconds=3)\n",
    "    if datetime.datetime.now()-timer > delta:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env_name, replay_memory_size=150_000, max_frames=10_000_000, gamma=0.99, batch_size=32,  \\\n",
    "            learning_rate=0.00025, sync_target_frames=10_000, net_update=4, replay_start_size=50_000, \\\n",
    "            eps_start=1, eps_min=0.1, seed=2109, device='cuda', verbose=True):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento.\n",
    "    \"\"\"\n",
    "    parameters = \"Environment: {} \\\n",
    "                 \\nReplay Memory Size: {} \\\n",
    "                 \\nMax Frames: {} \\\n",
    "                 \\nGamma: {} \\\n",
    "                 \\nBatch Size: {} \\\n",
    "                 \\nLearning Rate: {} \\\n",
    "                 \\nSync Target Frames: {} \\\n",
    "                 \\nNet Update: {} \\\n",
    "                 \\nReplay Start Size: {} \\\n",
    "                 \\nInitial Epsilon: {} \\\n",
    "                 \\nMinimum Epsilon: {} \\\n",
    "                 \\nRandom Seed: {}\".format(env_name,replay_memory_size,max_frames,gamma,batch_size,learning_rate,sync_target_frames,\n",
    "                                                 net_update,replay_start_size,eps_start,eps_min,seed)\n",
    "\n",
    "    path = \"dictsOpt/\" + env_name + \"_opt\"\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    aux_file = path + \"/\" + env_name + \"_parameters_opt.txt\"\n",
    "    with open(aux_file, 'w+') as f:\n",
    "        f.write(parameters)\n",
    "    \n",
    "    env = make_atari(env_name, max_steps=1_000)\n",
    "    buffer = ExperienceReplay(replay_memory_size)\n",
    "    agent = Agent(env, buffer)\n",
    "    set_seed(seed=seed, env=env)\n",
    "    \n",
    "    net        = DQN((4,84,84), env.action_space.n).to(device)\n",
    "    target_net = DQN((4,84,84), env.action_space.n).to(device)\n",
    "    \n",
    "    epsilon = eps_start\n",
    "    eps_decay = (eps_start - eps_min) / replay_memory_size\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    total_rewards = []\n",
    "    loss_history = []\n",
    "\n",
    "    best_mean_reward = None\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    for frame in tqdm(range(1, max_frames+1), desc=env_name):\n",
    "        start_frame = datetime.datetime.now()\n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device)\n",
    "\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            \n",
    "            time_passed = datetime.datetime.now() - start_time\n",
    "            \n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), path + \"/\" + env_name + \"_opt_best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "\n",
    "        epsilon = max(epsilon-eps_decay, eps_min)\n",
    "\n",
    "        if frame % net_update == 0:\n",
    "            sardn = buffer.sample(batch_size)\n",
    "            batch = Experience(*zip(*sardn))\n",
    "            \n",
    "            states_v = torch.tensor(np.array(batch.state)).to(device)\n",
    "            next_states_v = torch.tensor(np.array(batch.next_state)).to(device)\n",
    "            actions_v = torch.tensor(batch.action).to(device)\n",
    "            rewards_v = torch.tensor(batch.reward).to(device)\n",
    "            done_mask = torch.BoolTensor(batch.done).to(device)\n",
    "            \n",
    "            state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "            next_state_values = target_net(next_states_v).max(1)[0]\n",
    "            next_state_values[done_mask] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "            expected_state_action_values = next_state_values*gamma + rewards_v\n",
    "            \n",
    "            loss_t = nn.HuberLoss()(state_action_values, expected_state_action_values) # MSELoss()(input,target)\n",
    "            \n",
    "            loss_history.append(loss_t.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss_t.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if frame % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        if frame % (max_frames / 10) == 0:\n",
    "            if verbose:\n",
    "                print(\"{}:  {} games, best result {:.3f}, mean reward {:.3f}, eps {:.2f}, time {}\".format(\n",
    "                    frame, len(total_rewards), max(total_rewards), mean_reward, epsilon, time_passed))\n",
    "            torch.save(net.state_dict(), path + \"/\" + env_name + \"_opt_\" + str(int((frame)/(max_frames/10))) + \".dat\")\n",
    "\n",
    "        if episode_stopping(start_frame):\n",
    "            print('Taking too long')\n",
    "            break\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    print(\"{}:  {} games, mean reward {:.3f}, eps {:.2f}, time {}\".format(\n",
    "            frame, len(total_rewards), mean_reward, epsilon, time_passed))\n",
    "         \n",
    "    aux_file = path + env_name + \"_total_opt.pkl\"\n",
    "    with open(aux_file, 'wb+') as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    aux_file = path + env_name + \"_loss_opt.pkl\"\n",
    "    with open(aux_file, 'wb+') as f:\n",
    "        pickle.dump(loss_history, f)\n",
    "    return total_rewards, loss_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optical_flow(obs):\n",
    "        assert np.array(obs).shape == (2, 84, 84)\n",
    "\n",
    "        first_frame = np.array(obs)[0].astype('uint8')\n",
    "        #prev_gray = cv.cvtColor(first_frame, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "        mask = np.zeros((84,84,3))\n",
    "        mask[..., 1] = 255\n",
    "\n",
    "        frame = np.array(obs)[1].astype('uint8')\n",
    "        #gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "        flow = cv.calcOpticalFlowFarneback(first_frame, frame,\n",
    "                                        None, \n",
    "                                        0.5, 5, 5, 5, 7, 1.5, 0)\n",
    "\n",
    "        magnitude, angle = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "        mask[..., 2] = cv.normalize(magnitude, None, 0, 255, cv.NORM_MINMAX)\n",
    "\n",
    "        flow = cv.cvtColor(mask.astype('float32'), cv.COLOR_HSV2RGB)\n",
    "\n",
    "        final = np.zeros((4,84,84))\n",
    "        final[0:3] = flow.reshape(3,84,84)\n",
    "        final[3] = np.array(obs[1]) / 255.0\n",
    "\n",
    "        assert final.shape == (4,84,84)\n",
    "\n",
    "        return final.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(env_name, model_name, n_samples=100, verbose=True):\n",
    "    '''\n",
    "    Obtiene 'n_samples' muestras de la red entrenada.\n",
    "    '''\n",
    "    env_name = env_name + 'NoFrameskip-v4'\n",
    "    model = 'dicts/' + env_name + '/' + env_name + '_'+ str(model_name) + '.dat'\n",
    "    env = make_atari(env_name, sample=True)\n",
    "    net = DQN((4,84,84), env.action_space.n)\n",
    "    net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "    \n",
    "    rewards = np.zeros(n_samples)\n",
    "\n",
    "    for i in tqdm(range(n_samples), desc=model):\n",
    "    \n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            state = optical_flow(state)\n",
    "            state_v = torch.tensor(np.array([state], copy=False))\n",
    "            q_vals = net(state_v).data.numpy()[0]\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print('Game: {}, Reward: {}'.format(i+1,total_reward))\n",
    "\n",
    "        rewards[i] = total_reward\n",
    "\n",
    "    print(sum(rewards)/n_samples)\n",
    "    return rewards, sum(rewards)/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_1.dat: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_2.dat: 100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_3.dat: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_4.dat: 100%|██████████| 10/10 [00:09<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_5.dat: 100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_6.dat: 100%|██████████| 10/10 [00:09<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_7.dat: 100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_8.dat: 100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_9.dat: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dicts/MsPacmanNoFrameskip-v4/MsPacmanNoFrameskip-v4_10.dat: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "GAME_NAME = 'MsPacman'\n",
    "for i in range(10):\n",
    "    _, rw = sample(GAME_NAME, i+1, n_samples=10, verbose=False)\n",
    "    rewards.append(rw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env_name, model_name):\n",
    "    '''\n",
    "    Esta función genera un video de una corrida generada por la red entrenada.\n",
    "    '''\n",
    "    model = 'dicts/' + env_name + '/' + model_name\n",
    "    name_video = 'videos/Optical/videotest'\n",
    "    \n",
    "    env = make_atari(env_name, sample=True, render_mode='rgb_array')\n",
    "    video = None\n",
    "    video = VideoRecorder(env, base_path=name_video, enabled=True)\n",
    "\n",
    "    net = DQN((4,84,84), env.action_space.n)\n",
    "    net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    frame = 1\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        state = optical_flow(state)\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        env.render()\n",
    "        video.capture_frame()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if reward > 0:\n",
    "            print('{}, reward {}'.format(frame, reward))\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        frame += 1\n",
    "        \n",
    "    print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "    video.close()\n",
    "    video.enabled = False\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67, reward 10.0\n",
      "70, reward 10.0\n",
      "74, reward 10.0\n",
      "77, reward 10.0\n",
      "81, reward 10.0\n",
      "84, reward 10.0\n",
      "87, reward 10.0\n",
      "91, reward 10.0\n",
      "94, reward 10.0\n",
      "97, reward 10.0\n",
      "101, reward 10.0\n",
      "105, reward 10.0\n",
      "108, reward 10.0\n",
      "115, reward 10.0\n",
      "118, reward 10.0\n",
      "122, reward 10.0\n",
      "126, reward 10.0\n",
      "129, reward 10.0\n",
      "132, reward 10.0\n",
      "136, reward 10.0\n",
      "139, reward 10.0\n",
      "Total reward: 210.00\n"
     ]
    }
   ],
   "source": [
    "make_video(\"MsPacmanNoFrameskip-v4\", os.listdir('dicts/'+\"MsPacmanNoFrameskip-v4\")[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERDIDA DE INFORMACION AL MUESTREAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56d75c766439ed33c340fac9b6eafde55c344e7633c04b31119024beb6127a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
