{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478e353e-4f18-4413-9baa-9db9ff11f0d5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59ec3ece-a29f-4731-882b-77ba78953117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import gym\n",
    "import gym.spaces \n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import random\n",
    "import pickle\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3327993b-d3d0-4d73-a879-4c9749361ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, env):\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe36eae-4c9c-488f-9f18-eca737cad1bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f42e0b9-c15f-4dcc-9ffa-213a2ec276c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Realiza un número random de \"NOOP\" al invocar reset().\n",
    "    \"\"\"\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.noop_action = 0\n",
    "        assert self.env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "        \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        noops = self.env.unwrapped.np_random.integers(1, self.noop_max+1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de89a3e9-c4fa-469c-9174-86317231886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Salta un número de frames y regresa el valor promedio de cada pixel.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._skip = skip\n",
    "        self._obs_buffer = np.zeros((self._skip,) + self.env.observation_space.shape, dtype=np.uint8)\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, _, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs            \n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711908e6-322f-4e6b-ad73-51dc2b377b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLimit(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Termina el episodio después de un número de pasos.\n",
    "    Evita que los ambientes entren en un loop o sin moverse.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_episode_steps=None):\n",
    "        super(TimeLimit, self).__init__(env)\n",
    "        self._max_episode_steps = max_episode_steps\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self._elapsed_steps += 1\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            done = True\n",
    "            info['TimeLimit.truncated'] = True\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._elapsed_steps = 0\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8bb6b5-9df3-472a-bf81-ec108e9c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Realiza la acción \"FIRE\" para iniciar los juegos que lo requieran.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = self.env.ale.lives()\n",
    "        assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(self.env.unwrapped.get_action_meanings()) >= 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.lives > self.env.ale.lives():\n",
    "            self.lives = self.env.ale.lives()\n",
    "            action = 1\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e533d3-8571-42d2-836a-1c1cec1b37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_r=-1, max_r=1):\n",
    "        super().__init__(env)\n",
    "        self.min_r = min_r\n",
    "        self.max_r = max_r\n",
    "        \n",
    "    def reward(self, reward):\n",
    "        if reward < 0:\n",
    "            return -1\n",
    "        elif reward > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        return obs, self.reward(rew), done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ec89ce-9828-47f9-93ed-8547d7b5f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reescala las imágenes a 84x84 y las pasa de RGB a gris.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, width=84, height=84):\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        num_colors = 1\n",
    "        \n",
    "        new_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255,\n",
    "            shape = (self._height, self._width, num_colors),\n",
    "            dtype = np.uint8,\n",
    "        )\n",
    "        original_space = self.observation_space\n",
    "        self.observation_space = new_space\n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        frame = obs\n",
    "        frame = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "        frame = cv.resize(\n",
    "            frame, (self._width, self._height), interpolation=cv.INTER_AREA\n",
    "        )\n",
    "        obs = frame\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return self.observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce847a53-2766-462e-8f79-85de2194a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Reescala de 0-255 a 0-1.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255,\n",
    "            shape = self.env.observation_space.shape,\n",
    "            dtype = np.float32,\n",
    "        )\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return self.observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89e9eb3e-984b-4714-b2c6-789d62ef07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Apila los últimos k frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, k=4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low = 0,\n",
    "            high = 255,\n",
    "            shape = (4,84,84),\n",
    "            dtype = self.env.observation_space.dtype,\n",
    "        )\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return self.frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b33409-b0d0-439f-9bc2-35db801475d8",
   "metadata": {},
   "source": [
    "### Creación del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70da62c-f3df-4943-8b3b-42b361268712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_atari(env_id, frames=4, max_episode_steps=None, noop_max=30, skip=4):\n",
    "    \"\"\"\n",
    "    Crea el ambiente especificado, pasándolo por los Wrappers especificados.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id, render_mode=None)\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max)\n",
    "    env = MaxAndSkipEnv(env, skip)\n",
    "    if max_episode_steps is not None:\n",
    "        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ClipReward(env)\n",
    "    env = WarpFrame(env)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    env = FrameStack(env, frames)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c25944-8d3a-4f0e-a183-a0b581b79deb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e74e14-f53c-4b23-9002-007548cd4ccb",
   "metadata": {},
   "source": [
    "GAME = 'MsPacmanNoFrameskip-v4'\n",
    "env = make_atari(GAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3297e-b7a6-4c8d-920b-af1dda365b8c",
   "metadata": {},
   "source": [
    "start = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a0c25-97b2-48e2-8da4-a950910e4d60",
   "metadata": {},
   "source": [
    "start[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fa922-13b0-4505-a339-2a89b07e7da9",
   "metadata": {},
   "source": [
    "plt.imshow(start[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1148fc1-fed2-4973-a5fd-f66e711542da",
   "metadata": {},
   "source": [
    "state, _, _, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac540b99-2b34-403d-beaf-e32984d512a5",
   "metadata": {},
   "source": [
    "plt.imshow(state[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81aa43-e31f-4e44-919f-07e5b75d5d18",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4238987-bb22-4dee-945b-ad609eb4fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Profunda de Aprendizaje Q (Deep Q Network).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1,*shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd458e8-f053-4fb9-8535-5c8bc9a49bc1",
   "metadata": {},
   "source": [
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8cd2d-80e9-4c00-9e70-ade3bab6eb42",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4663546d-b698-40fb-80c9-ea49a268b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \"\"\"\n",
    "    Almacena experiencias pasadas que han sido observadas por el agente.\n",
    "    Las muestras obtenidas sirven para entrenar la red, buscando minimizar el efecto que tiene la correlación entre pasos.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, *args):\n",
    "        self.buffer.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edda062-f6e5-45dd-b260-d9996ac63aac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726aaf87-6793-4f0f-ad3b-e6ccb73adac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.total_reward = 0.0\n",
    "        \n",
    "    def play_step(self, net, epsilon=0.0, device='cuda'):\n",
    "        done_reward = None\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1) # Devuelve el índice de la acción\n",
    "            action = int(act_v.item())\n",
    "        \n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.exp_buffer.append(self.state, action, reward, done, new_state)\n",
    "        self.state = new_state\n",
    "        \n",
    "        if done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        \n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccc22b-9709-4ced-9e5e-f897d3d13eb8",
   "metadata": {},
   "source": [
    "### Función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b4f740f-3408-4391-b210-26373cbec3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env_name, replay_memory_size=1_000_000, max_frames=50_000_000, gamma=0.99, batch_size=32,  \\\n",
    "             learning_rate=0.00025, momentum=0.95, min_gradient=0.1, sync_target_frames=10_000, \\\n",
    "             replay_start_size=50_000, eps_start=1, eps_min=0.1, seed=2109, device='cuda', verbose=True):\n",
    "    \"\"\"\n",
    "    Función de entrenamiento.\n",
    "    \"\"\"\n",
    "    path = \"dicts/\" + env_name \n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    env = make_atari(env_name)\n",
    "    buffer = ExperienceReplay(replay_memory_size)\n",
    "    agent = Agent(env, buffer)\n",
    "    set_seed(seed=seed, env=env)\n",
    "    \n",
    "    net        = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    writer = SummaryWriter(comment='-' + env_name)\n",
    "    \n",
    "    epsilon = eps_start\n",
    "    eps_decay = (eps_start - eps_min) / replay_memory_size\n",
    "    \n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, momentum=momentum, eps=min_gradient)\n",
    "    total_rewards = []\n",
    "    \n",
    "    best_mean_reward = None\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    for frame in tqdm(range(1, max_frames+1)):\n",
    "        epsilon = max(epsilon-eps_decay, eps_min)\n",
    "        \n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            \n",
    "            time_passed = datetime.datetime.now() - start_time\n",
    "            \n",
    "            #writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "            writer.add_scalar(\"reward\", reward, frame)\n",
    "            \n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), path + \"/\" + env_name + \"_best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "                \n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "            \n",
    "        sardn = buffer.sample(batch_size)\n",
    "        batch = Experience(*zip(*sardn))\n",
    "        \n",
    "        states_v = torch.tensor(np.array(batch.state)).to(device)\n",
    "        next_states_v = torch.tensor(np.array(batch.next_state)).to(device)\n",
    "        actions_v = torch.tensor(batch.action).to(device)\n",
    "        rewards_v = torch.tensor(batch.reward).to(device)\n",
    "        done_mask = torch.BoolTensor(batch.done).to(device)\n",
    "        \n",
    "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "        expected_state_action_values = next_state_values*gamma + rewards_v\n",
    "        \n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values) # MSELoss()(input,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (frame + 1) % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        if (frame + 1) % (max_frames / 50) == 0:\n",
    "            if verbose:\n",
    "                print(\"{}:  {} games, best result {:.3f}, mean reward {:.3f}, eps {:.2f}, time {}\".format(\n",
    "                    frame + 1, len(total_rewards), max(total_rewards), mean_reward, epsilon, time_passed))\n",
    "        if (frame+1) % (max_frames/10) == 0:\n",
    "            torch.save(net.state_dict(), path + \"/\" + env_name + \"_\" + str(int((frame+1)/(max_frames/10))) + \".dat\")\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    print(\"{}:  {} games, mean reward {:.3f}, eps {:.2f}, time {}\".format(\n",
    "            frame + 1, len(total_rewards), mean_reward, epsilon, time_passed))\n",
    "         \n",
    "    writer.close()\n",
    "    pkl_file = \"dicts/\" + env_name + \"/\" + env_name + \".pkl\"\n",
    "    with open(pkl_file, 'wb+') as f:\n",
    "        pickle.dump(total_rewards, f)\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb97e3-be17-430e-b91d-6696bd57ee48",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24d99c90-ffb2-4e22-8544-bcae1903bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_NAME = \"MsPacmanNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b159668-05e4-48fc-907e-ea7625ee0b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:04<00:00, 1103.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n",
      "5001:  10 games, mean reward 20.200, eps 1.00, time 0:00:04.280481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pacman = training(env_name=GAME_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "67d2acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = np.array(list(gym.envs.registry.keys()))\n",
    "envs_mask1 = ['ramNoFrameskip' not in k for k in envs]\n",
    "envs = envs[envs_mask1]\n",
    "envs_mask2 = ['NoFrameskip-v4' in k for k in envs]\n",
    "valid_envs = envs[envs_mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff27c0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AdventureNoFrameskip-v4', 'AirRaidNoFrameskip-v4',\n",
       "       'AlienNoFrameskip-v4', 'AmidarNoFrameskip-v4',\n",
       "       'AssaultNoFrameskip-v4', 'AsterixNoFrameskip-v4',\n",
       "       'AsteroidsNoFrameskip-v4', 'AtlantisNoFrameskip-v4',\n",
       "       'BankHeistNoFrameskip-v4', 'BattleZoneNoFrameskip-v4',\n",
       "       'BeamRiderNoFrameskip-v4', 'BerzerkNoFrameskip-v4',\n",
       "       'BowlingNoFrameskip-v4', 'BoxingNoFrameskip-v4',\n",
       "       'BreakoutNoFrameskip-v4', 'CarnivalNoFrameskip-v4',\n",
       "       'CentipedeNoFrameskip-v4', 'ChopperCommandNoFrameskip-v4',\n",
       "       'CrazyClimberNoFrameskip-v4', 'DefenderNoFrameskip-v4',\n",
       "       'DemonAttackNoFrameskip-v4', 'DoubleDunkNoFrameskip-v4',\n",
       "       'ElevatorActionNoFrameskip-v4', 'EnduroNoFrameskip-v4',\n",
       "       'FishingDerbyNoFrameskip-v4', 'FreewayNoFrameskip-v4',\n",
       "       'FrostbiteNoFrameskip-v4', 'GopherNoFrameskip-v4',\n",
       "       'GravitarNoFrameskip-v4', 'HeroNoFrameskip-v4',\n",
       "       'IceHockeyNoFrameskip-v4', 'JamesbondNoFrameskip-v4',\n",
       "       'JourneyEscapeNoFrameskip-v4', 'KangarooNoFrameskip-v4',\n",
       "       'KrullNoFrameskip-v4', 'KungFuMasterNoFrameskip-v4',\n",
       "       'MontezumaRevengeNoFrameskip-v4', 'MsPacmanNoFrameskip-v4',\n",
       "       'NameThisGameNoFrameskip-v4', 'PhoenixNoFrameskip-v4',\n",
       "       'PitfallNoFrameskip-v4', 'PongNoFrameskip-v4',\n",
       "       'PooyanNoFrameskip-v4', 'PrivateEyeNoFrameskip-v4',\n",
       "       'QbertNoFrameskip-v4', 'RiverraidNoFrameskip-v4',\n",
       "       'RoadRunnerNoFrameskip-v4', 'RobotankNoFrameskip-v4',\n",
       "       'SeaquestNoFrameskip-v4', 'SkiingNoFrameskip-v4',\n",
       "       'SolarisNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4',\n",
       "       'StarGunnerNoFrameskip-v4', 'TennisNoFrameskip-v4',\n",
       "       'TimePilotNoFrameskip-v4', 'TutankhamNoFrameskip-v4',\n",
       "       'UpNDownNoFrameskip-v4', 'VentureNoFrameskip-v4',\n",
       "       'VideoPinballNoFrameskip-v4', 'WizardOfWorNoFrameskip-v4',\n",
       "       'YarsRevengeNoFrameskip-v4', 'ZaxxonNoFrameskip-v4'], dtype='<U36')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4b1b7-5e35-4e37-b157-70ab9bd3e676",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Muestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85221452-1373-4f41-bcb3-9c68f1623ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(env_name, n_samples=20):\n",
    "    '''\n",
    "    Obtiene 'n_samples' muestras de la red entrenada.\n",
    "    '''\n",
    "    model = 'dicts/' + env_name + \"/\" + env_name + '_best.dat'\n",
    "\n",
    "    env = make_atari(env_name)\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "    net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "    \n",
    "    rewards = np.zeros(n_samples)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "    \n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            state_v = torch.tensor(np.array([state], copy=False))\n",
    "            q_vals = net(state_v).data.numpy()[0]\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards[i] = total_reward\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4930052-cbeb-40ca-b853-4f37fb91ab21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-21., -21., -21., -21., -21., -21., -21., -21., -21., -21., -21.,\n",
       "       -21., -21., -21., -21., -21., -21., -21., -21., -21.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(GAME_NAME, n_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089732c-8828-4fda-9799-0360d60aa805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Tesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "56d75c766439ed33c340fac9b6eafde55c344e7633c04b31119024beb6127a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
